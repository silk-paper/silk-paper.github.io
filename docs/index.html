<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>SILK: Smooth Interpolation Framework for Motion In-betweening</title>
    <meta name="description" content="Simple yet powerful transformer-based model for solving motion in-betweening tasks">

    <meta property="og:title" content="SILK: Smooth Interpolation Framework for Motion In-betweening">
    <meta property="og:description" content="Simple yet powerful transformer-based model for solving motion in-betweening tasks">
    <meta property="og:image" content="https://silk-paper.github.io/silk/static/assets/teaser.png">
    <meta property="og:type" content="website">
    <meta name="twitter:card" content="https://silk-paper.github.io/silk/static/assets/teaser.png">
    <meta property="og:url" content="https://silk-paper.github.io/silk/">
    <link rel="canonical" href="https://silk-paper.github.io/silk/">

    <style>
      :root {
        --accent-color: rgb(6, 69, 173);
        --fg-color: rgb(32, 33, 34);
        --bg-color: #fff;
        --max-content-width: 950px;
      }

      body {
        font-family: sans-serif;
        font-size: calc(12px + 0.05vw);
        color: var(--fg-color);
        background-color: var(--bg-color);
        font-weight: 400;
        -webkit-font-smoothing: antialiased;
      }

      main {
        margin: 0 auto;
        width: 90vw;
        max-width: var(--max-content-width);
        padding: 2rem 0;
        background-color: var(--bg-color);
        min-height: 90vh;
      }

      h1, h2 {
        margin-top: 1.3em;
      }

      h1 {
        font-weight: 400;
        font-size: 2.2em;
      }

      h2 {
        font-weight: 600;
        font-size: 1.9em;
      }

      h3 {
        font-weight: 400;
        font-size: 2.2em;
        text-align: center;;
      }

      p, li {
        font-size: 1.3em;
        line-height: 1.6;
      }

      a {
        text-decoration: none;
        color: var(--accent-color);
      }

      a:hover {
        text-decoration: underline;
      }

      img {
        width: 100%;
        height: auto;
      }

      video {
        position: absolute;
        width: 100%;
        height: 100%;
      }

      button {
        padding: 10px 20px;
        font-size: 16px;   
      }
    </style>

  </head>
  <body>
    <main>
      <h1 style="margin-top: 0;"><strong>SILK</strong>: <b>S</b>mooth <b>I</b>nterpo<b>L</b>ation framewor<b>K</b> for motion in-betweening</h1>
      <div>
        <span style="font-style: italic; font-size: 1.3em;">In submission</span>
        <p style="line-height: 1.5;">
          <span style="display: block; text-transform: uppercase;">Anonymous Authors</span>
        </p>
      </div>
      <p>
        <picture>
          <source srcset="static/assets/teaser.avif" type="image/avif">
          <source srcset="static/assets/teaser.webp" type="image/webp">
          <img alt="teaser-silk" src="static/assets/teaser.png" width="2914" height="1016">
        </picture>
      </p>

      <h2>Abstract</h2>
      <p>
        Motion in-betweening is a crucial tool for animators, enabling intricate control over pose-level details in each key frame. In this work, we introduce a simple yet effective Transformer-based framework (SILK) for synthesizing realistic motions for motion in-betweening tasks. We show that the choice of pose representation significantly impacts the quality of the synthesized motions. Our model, a standard Transformer encoder, consistently matches or exceeds the performance of existing learning-based solutions in all metrics on the LaFAN1 benchmark and in our user study. SILK demonstrates superior generalization capabilities to longer sequences and outperforms alternatives when extrapolating to an unseen number of in-betweening frames. Moreover, we address the absence of a consistent methodology in literature for evaluating learning-based motion in-betweening solutions by introducing a cross-validation setup for the LaFAN1 dataset.
      </p>

      <section>
        <h2>Internal Soccer Motion Results</h2>
        <p>
          We train SILK on a larger and noisier dataset.
          The goal is to evaluate its capabilities when trained on motion capture data that better reflects average quality.
          The dataset contains around 40 hours of mixed quality soccer movements.
          Videos below show SILK's predictions on some of the held-out testing motion sequences.
        </p>

        <p>
          The videos are shown at half speed (50%) by default.
        </p>

        <div style="text-align: center;">
          <button onclick="(() => (document.querySelectorAll('video').forEach((e) => e.playbackRate = 2.0)))();">100%</button>
          <button onclick="(() => (document.querySelectorAll('video').forEach((e) => e.playbackRate = 1.0)))();">50%</button>
          <button onclick="(() => (document.querySelectorAll('video').forEach((e) => e.playbackRate = 0.5)))();">25%</button>
        </div>
        <div style="position: relative; padding-bottom: 56.25%;">
          <video controls muted loop>
            <source src="static/results/anim10.mp4" type="video/mp4">
          </video>
        </div>
        <div style="position: relative; padding-bottom: 56.25%;">
          <video controls muted loop>
            <source src="static/results/anim11.mp4" type="video/mp4">
          </video>
        </div>
        <div style="position: relative; padding-bottom: 56.25%;">
          <video controls muted loop>
            <source src="static/results/anim12.mp4" type="video/mp4">
          </video>
        </div>
        <div style="position: relative; padding-bottom: 56.25%;">
          <video controls muted loop>
            <source src="static/results/anim13.mp4" type="video/mp4">
          </video>
        </div>
        <div style="position: relative; padding-bottom: 56.25%;">
          <video controls muted loop>
            <source src="static/results/anim14.mp4" type="video/mp4">
          </video>
        </div>
      </section>

      <section>
        <h2>Pose Representation Comparison – t-SNE Visualization</h2>
        <p>
          Based on both PCA and t-SNE plots, we see more distinct clusters in the data within the root-space, each associated with different animation types. Some action types have natural overlap, e.g., walking and running motions exist in other action classes, while others should be clearly distinct. Our hypothesis is that the more structured the data is in the reduced dimension, the easier it is for the model to make predictions.
        </p>

        <p>
          To quantify the clustering, we cluster the data in both the PCA and the t-SNE space using k-means and compute the v-score. The v-score assesses completeness and homogeneity of clusters with respect to the provided ground truth labels. For PCA, on root and local features, we find a v-score of 0.2317 and 0.191 respectively. For t-SNE, we record a v-score of 0.41 for root features and 0.32 for local features. 
        </p>

        <h3>PCA – Local-to-Parent Space</h3>
        <div style="position: relative;">
          <img src="static/results/local_space_pca.gif" width="2125" height="1224" loading="lazy" decoding="async">
        </div>

        <h3>PCA – Root Space</h3>
        <div style="position: relative;">
          <img src="static/results/root_space_pca.gif" width="2125" height="1224" loading="lazy" decoding="async">
        </div>

        <h3>t-SNE – Local-to-Parent Space</h3>
        <div style="position: relative;">
          <img src="static/results/local_space_tsne.gif" width="2125" height="1224" loading="lazy" decoding="async">
        </div>

        <h3>t-SNE – Root Space</h3>
        <div style="position: relative;">
          <img src="static/results/root_space_tsne.gif" width="2125" height="1224" loading="lazy" decoding="async">
        </div>
      </section>

  </body>
</html>